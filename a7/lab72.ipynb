{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from blackjack import BlackjackEnv\n",
    "import plotting\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlackjackEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n",
    "    \n",
    "    Args:\n",
    "        Q: A dictionary that maps from state -> action-values.\n",
    "            Each value is a numpy array of length nA (see below)\n",
    "        epsilon: The probability to select a random action . float between 0 and 1.\n",
    "        nA: Number of actions in the environment.\n",
    "    \n",
    "    Returns:\n",
    "        A function that takes the observation as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "    \n",
    "    \"\"\"\n",
    "    def policy_fn(observation):\n",
    "        # np.random.seed(0)\n",
    "        optimal_action = np.argmax(Q[observation])\n",
    "        pi = np.full(nA, epsilon / nA)\n",
    "        pi[optimal_action] = epsilon / nA + 1 - epsilon\n",
    "        \n",
    "        return pi\n",
    "    return policy_fn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why nested function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "def mc_control_epsilon_greedy(env, num_episodes, discount_factor=1.0, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Monte Carlo Control using Epsilon-Greedy policies.\n",
    "    Finds an optimal epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI gym environment.\n",
    "        num_episodes: Number of episodes to sample.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        epsilon: Chance the sample a random action. Float betwen 0 and 1.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, policy).\n",
    "        Q is a dictionary mapping state -> action values.\n",
    "        policy is a function that takes an observation as an argument and returns\n",
    "        action probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    # state -> action-value array (whose index is action). \n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n)) # initial policy evaluation\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n) # initial policy improvement\n",
    "    action_space = list(range(env.action_space.n))\n",
    "    \n",
    "    # Generate episodes (i.e. full blackjack games)\n",
    "    Experience = namedtuple('Experience', ['state', 'action', 'reward'])\n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "            \n",
    "        # Generate experiences (state, action, reward); store in list 'experiences'\n",
    "        experiences = []\n",
    "        state = env.reset()\n",
    "        for t in range(100):\n",
    "            action = np.random.choice(action_space, p=policy(state))\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            experiences.append(Experience(state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "            \n",
    "        # Compute average returns in states experienced in this episode\n",
    "        Q = evaluate_policy(experiences, returns_sum, returns_count, Q, discount_factor)\n",
    "        policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "        \n",
    "    #total number of policy improvement cycles\n",
    "    print(\"policy improvement \", sum(returns_count.values()))\n",
    "    return Q, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(experiences, returns_sum, returns_count, Q, discount_factor):\n",
    "    experienced_state_actions = set([(experience.state, experience.action) for experience in experiences])\n",
    "    for state_action in experienced_state_actions:\n",
    "        # Find the first occurance of the state in the episode\n",
    "        first_occurence_idx = next(i for i, experience in enumerate(experiences) if (experience.state, experience.action) == state_action)\n",
    "        # Sum up all rewards since the first occurance\n",
    "        G = sum([experience.reward * (discount_factor**i) for i, experience in enumerate(experiences[first_occurence_idx:])])\n",
    "        # Calculate average return for this state over all sampled episodes\n",
    "        returns_sum[state_action] += G\n",
    "        returns_count[state_action] += 1.0\n",
    "            \n",
    "        s, a = state_action\n",
    "        Q[s][a] = returns_sum[state_action] / returns_count[state_action]\n",
    "        \n",
    "    return Q\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ludde/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy improvement  1276598.0\n",
      "\n",
      " 143.35381099999998 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ludde/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "start = time.clock()\n",
    "\n",
    "Q, policy = mc_control_epsilon_greedy(env, num_episodes=1000000, epsilon=0.1)\n",
    "\n",
    "end = time.clock()\n",
    "print(\"\\n\",end - start,\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'returns_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-37f7f4b4770e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# by picking the best action at each state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#total number of policy improvement cycles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"policy improvement \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturns_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'returns_count' is not defined"
     ]
    }
   ],
   "source": [
    "# For plotting: Create value function from action-value function\n",
    "# by picking the best action at each state\n",
    "#total number of policy improvement cycles\n",
    "print(\"policy improvement \", sum(returns_count.values()))\n",
    "V = defaultdict(float)\n",
    "for state, actions in Q.items():\n",
    "    action_value = np.max(actions)\n",
    "    V[state] = action_value\n",
    "plotting.plot_value_function(V, title=\"Optimal Value Function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
